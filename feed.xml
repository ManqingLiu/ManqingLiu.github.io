<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://manqingliu.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://manqingliu.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-01-09T19:36:54+00:00</updated><id>https://manqingliu.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">On integrating doubly robust estimators with MCTS and it’s connection with PPO</title><link href="https://manqingliu.github.io/blog/2025/DR-MCTS/" rel="alternate" type="text/html" title="On integrating doubly robust estimators with MCTS and it’s connection with PPO"/><published>2025-01-08T00:00:00+00:00</published><updated>2025-01-08T00:00:00+00:00</updated><id>https://manqingliu.github.io/blog/2025/DR-MCTS</id><content type="html" xml:base="https://manqingliu.github.io/blog/2025/DR-MCTS/"><![CDATA[<p>In this post, I will explore the <strong>theoretical</strong> justification for integrating doubly robust estimators into Monte Carlo Tree Search (MCTS) and discuss its relationship with how it relates to <a href="https://arxiv.org/abs/1707.06347">Proximal Policy Optimization (PPO)</a>. I welcome any feedback or comments on this post.</p> <h3 id="problem-statement">Problem Statement</h3> <p>MMonte Carlo Tree Search (MCTS) has emerged as a powerful algorithm for decision-making in complex, large-scale state spaces. Its success is evident in groundbreaking AI systems such as <a href="https://deepmind.google/research/breakthroughs/alphago/">AlphaGo</a>, <a href="https://deepmind.google/discover/blog/alphazero-shedding-new-light-on-chess-shogi-and-go/">AlphaZero</a>, and <a href="https://deepmind.google/technologies/alphafold/">AlphaFold</a>. With the recent advancements of large language models (LLMs) like <a href="https://openai.com/index/introducing-chatgpt-pro/">GPTs</a>, <a href="https://claude.ai/new">Claude</a>, and <a href="https://www.llama.com/">LLama</a> in the NLP community, there has been a growing interest in leveraging these models with MCTS frameworks.</p> <p>Recent research (<a href="https://arxiv.org/abs/2305.14078">Zhao et al. 2023</a>, <a href="https://arxiv.org/abs/2310.04406">Zhou et al. 2024</a> ) has explored the integration of LLMs into MCTS. It has been speculated that OpenAI’s <a href="https://openai.com/o1/">o1 model</a> employs MCTS, using the model’s confidence to guide the search and expand the solution space (<a href="https://arxiv.org/pdf/2411.14405v1">Zhao et al. 2024</a>). The core idea is to use the LLM’s world model to provide a prior belief of states, enabling more informed decision-making.</p> <p>Traditionally, MCTS relies on a rollout policy to estimate the value function, which guides the search towards relevant parts of the tree. However, this approach may be less effective in high-dimensional state spaces where accurate rollouts are challenging to obtain. While MCTS is primarily an on-policy algorithm, techniques such as importance sampling (IS) and doubly robust (DR) estimators—typically associated with off-policy evaluation—have not been widely applied to MCTS.</p> <p>Our research interest lies in integrating DR estimators into MCTS to enhance value function estimation and, consequently, improve the search’s success rate. This integration presents an intriguing opportunity to bridge the gap between on-policy learning in MCTS and off-policy evaluation techniques.</p> <h3 id="why-doubly-robust-estimators">Why Doubly Robust Estimators?</h3> <p><a href="https://academic.oup.com/biomet/article-abstract/82/4/805/252258">Doubly robust (DR) estimators</a> have gained significant popularity in causal inference due to their unique properties. Originally developed for missing data problems, DR estimators have found widespread use in observational studies and clinical trials. The key advantage of DR estimators is their ability to provide unbiased estimates if either the outcome model or the propensity score model is correctly specified, offering a safeguard against model misspecification.</p> <p>In causal inference, DR estimators combine two approaches:</p> <ol> <li>An outcome regression model that predicts the outcome based on covariates and treatment.</li> <li>A propensity score model that estimates the probability of receiving treatment given covariates.</li> </ol> <p>This dual modeling approach provides a “double” chance of getting the correct estimate, hence the term “doubly robust.”</p> <p>The success of DR estimators in causal inference has inspired researchers to adapt these methods to the reinforcement learning (RL) domain. Two seminal papers have been instrumental in this transition:</p> <ol> <li> <p><a href="https://arxiv.org/abs/1511.03722">Jiang and Li (2016)</a> introduced the idea of using DR estimators for off-policy evaluation in RL. Their work demonstrated how DR estimators could be applied to estimate the value of a target policy using data collected from a different behavior policy. This approach showed promising results in reducing variance and bias compared to traditional importance sampling methods.</p> </li> <li> <p><a href="https://arxiv.org/abs/1604.00923">Thomas and Brunskill (2016)</a> further developed this concept by proposing a more data-efficient DR estimator for off-policy evaluation. Their method, known as the weighted doubly robust estimator, incorporated adaptive importance sampling techniques to improve estimation accuracy, especially in scenarios with limited data.</p> </li> </ol> <p>These pioneering works established DR estimators as a powerful tool in RL, particularly for off-policy evaluation. The success of DR estimators in this context stems from their ability to leverage both the direct method (using a model of the environment) and importance sampling, combining the strengths of both approaches while mitigating their individual weaknesses.</p> <h3 id="integrating-doubly-robust-estimators-with-mcts">Integrating Doubly Robust Estimators with MCTS</h3> <p>We hypothesize that the nice properties of DR estimators can be preserved when integrated with MCTS.</p>]]></content><author><name></name></author><category term="research"/><category term="work-in-progress"/><category term="RL"/><category term="causality"/><category term="comments"/><summary type="html"><![CDATA[A blog post on integrating doubly robust estimators with MCTS and it's connection with PPO]]></summary></entry><entry><title type="html">The bitter lessons I learnt from my first PhD project</title><link href="https://manqingliu.github.io/blog/2025/my-first-blog/" rel="alternate" type="text/html" title="The bitter lessons I learnt from my first PhD project"/><published>2025-01-03T00:00:00+00:00</published><updated>2025-01-03T00:00:00+00:00</updated><id>https://manqingliu.github.io/blog/2025/my-first-blog</id><content type="html" xml:base="https://manqingliu.github.io/blog/2025/my-first-blog/"><![CDATA[<p>I recently completed my first PhD <a href="https://arxiv.org/abs/2410.10044">project</a>, and I must say it was a bittersweet experience. I learned a lot, but I also made plenty of mistakes. In this post, I’ll share the bitter lessons I picked up along the way.</p> <p>A quick disclaimer: I’m no expert in this field, so definitely read it with a grain of salt. I’m sharing my experience in the hope that it’ll help others avoid the pitfalls I encountered, especially those who, like me, don’t come from a traditional computer science background.</p> <h3 id="lesson-1-start-with-an-existing-repository-if-you-can-find-one-and-build-on-it">Lesson 1: Start with an existing repository if you can find one and build on it</h3> <p>When I kicked off my first PhD project, I decided to build everything from scratch. I thought it would give me a deeper understanding and more control. At the time, I was new to Python and had a deep-rooted fear of using or changing other people’s code.</p> <p>This turned out to be a massive mistake that cost me valuable time and energy. Looking back, I should have searched for existing repositories or codebases related to my research topic.</p> <p>Starting from an existing codebase would have saved me months of work setting up basic infrastructures and implementing standard algorithms. I could have learned best practices and coding standards specific to my field by examining and building upon existing code. Instead of reinventing the wheel, I could have devoted more time to developing and implementing my unique ideas.</p> <p>Of course, this doesn’t mean you should never build from scratch. There are times when it’s necessary or beneficial. But for most PhD projects, especially in the early stages, leveraging existing work can significantly accelerate your progress and allow you to focus on your unique contributions.</p> <h3 id="lesson-2-check-public-datasets-first-before-simulating-your-own-data">Lesson 2: Check public datasets first before simulating your own data</h3> <p>I was eager to create a custom dataset that perfectly matched my research needs. I spent months developing complex simulations to generate synthetic data. However, this decision came with several unforeseen challenges.</p> <p>Creating and validating a synthetic dataset took far longer than anticipated, especially when trying to create a dataset as close to real-world as possible. In the context of causal inference, simulating a data-generating process that considers all possible interactions among confounders, treatment, and outcome is incredibly challenging and time-consuming.</p> <p>Later, I discovered several public datasets with various numbers of covariates and ground truth causal effects that could have served my needs with minimal modifications. I had reinvented the wheel unnecessarily.</p> <p>In hindsight, I should have first thoroughly investigated existing public datasets. Many fields have well-established, high-quality public datasets that are regularly used and validated by the research community. Using these allows you to start your actual research faster and enables direct comparison of your methods with existing work.</p> <p>That’s not to say creating your own dataset is always wrong, especially for initial small-scale hypothesis testing. But consider public datasets when scaling up your research.</p> <h3 id="lesson-3-dont-be-over-reliant-on-generative-ai-for-debugging">Lesson 3: Don’t be over-reliant on generative AI for debugging</h3> <p>Pursuing a technical PhD in GenAI is like opening Pandora’s box. Initially, my workflow was a frustrating cycle: a terrifying red error message would pop up, I’d frantically consult ChatGPT, receive a solution I barely comprehended, attempt to implement it, only to face another error message. This cycle would repeat until, miraculously, the solution worked - leaving me none the wiser about why.</p> <p>While generative AI can be a powerful debugging tool, it’s a double-edged sword, especially for beginners. It’s dangerously easy to fall into the trap of mental laziness, allowing the AI to do all the thinking. This approach can lead to either getting the right solution or merely suppressing the error message while the underlying problem persists.</p> <p>I confess I still fall into this trap occasio nally. However, I’m striving to be more mindful of AI-provided solutions. My new approach is to always attempt solving the bug myself first, understanding the root cause, before considering the AI’s suggestion.</p> <p>In AI research, understanding the ‘why’ behind a solution is often more crucial than the solution itself. Let’s use AI as a tool to augment our problem-solving skills, not replace them.</p> <h3 id="lesson-4-dont-be-afraid-to-ask-for-help-even-if-you-think-you-should-know-the-answer">Lesson 4: Don’t be afraid to ask for help, even if you think you should know the answer</h3> <p>Just as I was tempted to let AI do all the thinking, I was also hesitant to seek human help. I thought I should know the answers, or I assumed my code was correct simply because it ran without throwing errors. This mindset, I’ve learned, can be incredibly dangerous in research.</p> <p>For the second part of my first project, which was a continuation of a <a href="https://arxiv.org/abs/2205.09824">paper</a> by former lab mates. One of the leading authors is <a href="https://davidbellamy.github.io/">David R. Bellamy</a> who is a brilliant researcher and a great mentor. I finally mustered the courage to reach out for guidance. This decision proved invaluable. When David reviewed my code, we uncovered a silent error - one that didn’t trigger any warnings but significantly impacted the results. Something as seemingly minor as the difference between <code class="language-plaintext highlighter-rouge">tensor.view()</code> and <code class="language-plaintext highlighter-rouge">tensor.permute()</code> led to substantial discrepancies in the output.</p> <p>The process of code review turned into much more than just error-hunting. It became an impromptu masterclass in coding standards, best practices, debugging techniques, and how to write clean, readable code. David’s willingness to help and share knowledge reminded me of a fundamental truth in academia: we’re all here to learn and grow together.</p> <h3 id="lesson-5-keep-a-daily-log-of-your-experiments">Lesson 5: Keep a daily log of your experiments</h3> <p>This lesson was profoundly influenced by <a href="http://joschu.net/">John Schulman</a>’s insightful <a href="http://joschu.net/blog/opinionated-guide-ml-research.html">blog post</a> on conducting ML research. I wish I’d stumbled upon it earlier in my PhD journey!</p> <p>From the outset, I diligently used <a href="https://wandb.ai/site/">Weights &amp; Biases</a> to log my experiments. However, I overlooked the crucial practice of maintaining a daily log. This oversight made tracking my progress a Herculean task. My workflow devolved into a haphazard cycle of tweaking parameters and re-running experiments without clear direction.</p> <p>One of the most insidious traps in my PhD journey was the illusion of time well spent. Sure, I was constantly running experiments, but without a daily log detailing my plans, code changes, results, and reflections, I was essentially stumbling around in the dark.</p> <p>So, let me implore you: keep a daily log of your experiments! Include your plan for the day, specific changes made to your code, results corresponding to each change, reflections and insights gained, and next steps based on these outcomes.</p> <p>This practice isn’t just about record-keeping; it’s about cultivating a mindful approach to your research. It will help you track progress, understand patterns, make informed decisions, avoid repeating unsuccessful approaches, and reflect on your growth as a researcher.</p> <h3 id="lesson-6-sometimes-a-bottom-up-approach-is-better-than-a-top-down-approach">Lesson 6: Sometimes a bottom up approach is better than a top-down approach</h3> <p>My first project aimed to leverage transformer models for causal effect estimation. Excited by the potential, I dove headfirst into using PyTorch’s ready-made <code class="language-plaintext highlighter-rouge">Transformer</code> model. Our idea seemed straightforward, but reality hit us like a ton of bricks.</p> <p>Our grand transformer model refused to converge, the loss stubbornly refused to decrease, and its performance lagged behind “simple” models like random forests or Multilayer Perceptron (MLP). I spent months tweaking the model and adjusting parameters, but the results remained disappointing.</p> <p>The breakthrough came when I decided to take a step back and adopt a bottom-up approach. I started by implementing a simple MLP model, gradually incorporating transformer components like self-attention layers. This incremental approach allowed me to understand the transformer’s inner workings, identify bottlenecks, and make targeted improvements.</p> <p>This experience taught me a valuable lesson: sometimes, a bottom-up approach is more effective than a top-down approach. By starting with simple, interpretable models and gradually incorporating complex components, you gain a deeper understanding of your model’s behavior and can make more informed decisions.</p> <h3 id="lesson-7-idea-is-cheap-the-devil-is-in-the-details">Lesson 7: Idea is cheap, the devil is in the details</h3> <p>Having an “original” idea is relatively easy. The real challenge lies in implementation and execution. You’ll never know if your intuition is correct until you’ve rigorously tested it, and often the result isn’t what you expected. This is what makes research both exciting and frustrating.</p> <p>In my experience, the gap between a promising idea and successful implementation is filled with countless hours of debugging, parameter tuning, and sometimes rethinking the entire approach. It’s in these details where true innovation often happens.</p> <p>I’ve learned that persistence is key, but so is flexibility. Sometimes, pushing forward means being willing to pivot your approach while still pursuing the core of your original idea. It’s about finding a balance between stubbornness in your vision and adaptability in your methods.</p> <p>Documentation and collaboration become your best friends in this process. Keep detailed notes and don’t be afraid to discuss your challenges with peers or mentors.</p> <p>Many groundbreaking discoveries came not from the initial idea, but from the persistent work that followed. The difference between a good idea and a great contribution often lies in your ability to tackle the nitty-gritty details with creativity and perseverance.</p> <p>So, the next time you’re stuck in the quagmire of implementation details, this might be where the real magic of research happens. Embrace the challenge, stay curious, and keep pushing forward. Your breakthrough might be just around the corner, hidden in the details you’re currently wrestling with.</p>]]></content><author><name></name></author><category term="blog-posts"/><category term="reflections"/><category term="comments"/><summary type="html"><![CDATA[A post about the lessons learnt from working on my first PhD project.]]></summary></entry></feed>