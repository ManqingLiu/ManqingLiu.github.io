<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://manqingliu.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://manqingliu.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-06-18T22:18:26+00:00</updated><id>https://manqingliu.github.io/feed.xml</id><title type="html">blank</title><subtitle>A simple, whitespace theme for academics. Based on [*folio](https://github.com/bogoli/-folio) design. </subtitle><entry><title type="html">On Representation</title><link href="https://manqingliu.github.io/blog/2025/on-representation/" rel="alternate" type="text/html" title="On Representation"/><published>2025-06-18T00:00:00+00:00</published><updated>2025-06-18T00:00:00+00:00</updated><id>https://manqingliu.github.io/blog/2025/on-representation</id><content type="html" xml:base="https://manqingliu.github.io/blog/2025/on-representation/"><![CDATA[<h1 id="on-representation-from-philosophy-to-mathematics-to-machine-learning">On Representation: From Philosophy to Mathematics to Machine Learning</h1> <h2 id="the-mystery-of-representation">The Mystery of “Representation”</h2> <p>What does <strong>representation</strong> mean? I encountered this word frequently in machine learning papers, where it sounds abstract and researchers assume you know what it is without explanation. The term appears across vastly different domains - philosophy, mathematics, and AI - yet seems to carry a deeper, unified meaning that connects these fields in surprising ways.</p> <h2 id="philosophical-foundations-from-plato-to-schopenhauer">Philosophical Foundations: From Plato to Schopenhauer</h2> <p>The concept of representation has deep philosophical roots. For <strong>Plato</strong>, representation (mimesis) refers to artistic imitations that are “copies of copies” - three degrees removed from truth since they imitate physical objects which themselves are merely imperfect copies of the eternal Forms (<em>Republic</em> Book X, 596a-598d). This establishes a hierarchy: the perfect Forms, their imperfect physical manifestations, and finally our representations of those manifestations.</p> <p><strong>Schopenhauer</strong> offers a different perspective. According to him, <strong>representation</strong> (German: <em>Vorstellung</em>) refers to the world as it appears to us through our cognitive faculties - essentially, the phenomenal world of experience as opposed to reality as it truly is. Where Plato sees representations as degraded copies, Schopenhauer sees them as the fundamental structure of how we can know anything at all.</p> <h2 id="mathematical-precision-the-riesz-representation-theorem">Mathematical Precision: The Riesz Representation Theorem</h2> <p>Perhaps not coincidentally, <strong>representation</strong> also appears in functional analysis, providing mathematical precision to these philosophical intuitions. The <strong>Riesz Representation Theorem</strong> serves as a perfect bridge between abstract philosophical concepts and concrete mathematical structures.</p> <h3 id="the-mathematical-foundation">The Mathematical Foundation</h3> <p><strong>Riesz Representation Theorem</strong>: For any continuous linear functional φ on a Hilbert space H, there exists a unique element f ∈ H such that:</p> \[\phi(g) = \langle f, g \rangle \text{ for all } g \in H\] <p>This establishes an isometric isomorphism between the space H and its dual H*:</p> <p>\(R: H^* \to H\) \(\phi \mapsto f \text{ where } \phi(\cdot) = \langle f, \cdot \rangle\)</p> <h3 id="the-philosophical-resonance">The Philosophical Resonance</h3> <p>The theorem embodies a profound philosophical principle: <strong>every way of measuring relationships (φ) has a unique concrete representation (f)</strong>. This mirrors Plato’s theory of Forms - abstract concepts like “justice” or “beauty” (the functionals φ) must have concrete manifestations (the representatives f) that preserve their essential relationships.</p> <p>What makes this connection profound is that the Riesz theorem guarantees <strong>uniqueness</strong> - there’s only one vector that can represent each functional. This suggests a mathematical foundation for the idea that abstract relationships have canonical concrete representations.</p> <h2 id="computational-platonism-ai-and-universal-representations">Computational Platonism: AI and Universal Representations</h2> <p>Two recent papers in AI reveal how these philosophical and mathematical ideas manifest in machine learning, suggesting we may be witnessing a form of computational Platonism.</p> <h3 id="the-platonic-representation-hypothesis">The Platonic Representation Hypothesis</h3> <p>“The Platonic Representation Hypothesis” by Huh et al. (2024) argues that representations in AI models are converging toward a shared statistical model of reality, explicitly drawing on Plato’s concept of an ideal reality that underlies our sensations. The authors demonstrate that as neural networks become larger and more capable, they develop increasingly similar ways of representing data, regardless of their different architectures, training objectives, or even data modalities.</p> <h3 id="harnessing-universal-geometry">Harnessing Universal Geometry</h3> <p>Building on this foundation, “Harnessing the Universal Geometry of Embeddings” by Jha et al. (2025) takes the bold step of proposing a “Strong Platonic Representation Hypothesis” - not only do these universal representations exist, but they can be practically exploited.</p> <p>Consider this striking example: imagine you’ve stolen a corporate vector database containing thousands of embedded emails, but you have no idea what the emails actually say or even how the embedding model works. The vec2vec method can take these mystery vectors and translate them into the space of a publicly available model like BERT, allowing you to extract sensitive information about the original emails - company names, financial discussions, even partial email content - without ever seeing the actual text or having any training examples that connect the two models.</p> <p>Their method achieves cosine similarities as high as 0.92 between translated and ground-truth embeddings, successfully extracting information from as many as 80% of documents in their experiments. This isn’t just a theoretical breakthrough; it’s a practical demonstration that the geometric structure of meaning itself may be more universal than we ever imagined.</p> <h2 id="the-mathematical-bridge-to-machine-learning">The Mathematical Bridge to Machine Learning</h2> <h3 id="embedding-spaces-as-hilbert-spaces">Embedding Spaces as Hilbert Spaces</h3> <p>In modern embedding spaces, the Riesz theorem manifests beautifully. Consider how semantic similarity is computed:</p> \[\text{similarity}(\text{text}_1, \text{text}_2) = \langle \text{embed}(\text{text}_1), \text{embed}(\text{text}_2) \rangle\] <p>Here, each embedded text acts as a “representation” of a linear functional that measures semantic affinity. The vec2vec translation can be viewed as finding representations across different Hilbert spaces:</p> <p>\(F: H_1 \to H_2\) \(\text{where } F(e_1) \approx e_2 \text{ such that } \langle F(e_1), F(e_1') \rangle_{H_2} \approx \langle e_1, e_1' \rangle_{H_1}\)</p> <h3 id="the-universal-latent-space">The Universal Latent Space</h3> <p>The “universal latent space” in vec2vec can be understood as the canonical Hilbert space where Riesz representatives take their most natural form. The method learns input adapters that transform embeddings from each encoder-specific space into a universal latent representation:</p> <p>\(A_1: H_1 \to H_{\text{universal}}\) \(A_2: H_2 \to H_{\text{universal}}\)</p> <p>where $H_{\text{universal}}$ contains the “true” Riesz representatives that both models are approximating.</p> <h2 id="the-deep-convergence">The Deep Convergence</h2> <p>What emerges is a remarkable convergence across domains:</p> <ul> <li> <p><strong>Philosophically</strong>: Both Plato and Schopenhauer suggest that our access to reality is mediated through representations that may reveal deeper truths about the structure of existence itself.</p> </li> <li> <p><strong>Mathematically</strong>: The Riesz theorem proves that abstract relationships must have unique concrete representations, providing the formal foundation for this intuition.</p> </li> <li> <p><strong>Computationally</strong>: Modern AI systems appear to be discovering these canonical representations empirically, converging on universal geometric structures that encode semantic meaning.</p> </li> </ul> <p>This suggests that when different AI systems converge on similar representations, they’re not just finding arbitrary solutions, but discovering the unique mathematical objects that faithfully encode semantic relationships. The universal geometry isn’t just an empirical observation - it’s a mathematical necessity, rooted in the fundamental structure of how abstract relationships must be concretely represented in any complete space.</p> <h2 id="representation-as-the-bridge-between-mind-and-reality">Representation as the Bridge Between Mind and Reality</h2> <p>The concept of representation serves as a bridge between the abstract and concrete, the universal and particular, the mathematical and empirical. From Plato’s Forms to Schopenhauer’s phenomenology, from the Riesz theorem to modern neural networks, we see the same fundamental question: How do abstract relationships manifest in concrete, computable forms?</p> <p>The recent work on universal embedding geometries suggests we may be witnessing a computational version of these age-old philosophical insights - different AI systems, like different coordinate systems in mathematics, may be discovering the same underlying representational structure of semantic space. This convergence implies that representation in machine learning isn’t just a useful computational trick, but may reflect fundamental principles about how complex structures can be efficiently encoded while preserving their essential relationships.</p> <p>Perhaps we are indeed living through a form of computational Platonism, where different AI systems are all glimpsing shadows of the same underlying forms - the canonical representations of meaning itself.</p> <h2 id="references">References</h2> <ul> <li>Huh, M., Cheung, B., Wang, T., &amp; Isola, P. (2024). The Platonic Representation Hypothesis. <em>arXiv preprint arXiv:2405.07987</em>.</li> <li>Jha, R., Zhang, C., Shmatikov, V., &amp; Morris, J. X. (2025). Harnessing the Universal Geometry of Embeddings. <em>arXiv preprint arXiv:2505.12540</em>.</li> <li>Plato. <em>Republic</em>. Book X, 596a-599a.</li> <li>Schopenhauer, A. <em>The World as Will and Representation</em>. Trans. E.F.J. Payne. 2 vols. New York: Dover Publications, 1969.</li> </ul>]]></content><author><name></name></author><summary type="html"><![CDATA[On Representation: From Philosophy to Mathematics to Machine Learning]]></summary></entry><entry><title type="html">On integrating doubly robust estimators with MCTS</title><link href="https://manqingliu.github.io/blog/2025/DR-MCTS/" rel="alternate" type="text/html" title="On integrating doubly robust estimators with MCTS"/><published>2025-01-08T00:00:00+00:00</published><updated>2025-01-08T00:00:00+00:00</updated><id>https://manqingliu.github.io/blog/2025/DR-MCTS</id><content type="html" xml:base="https://manqingliu.github.io/blog/2025/DR-MCTS/"><![CDATA[<p>In this post, I will explore the <strong>theoretical</strong> justification for integrating doubly robust estimators into Monte Carlo Tree Search (MCTS). I welcome any feedback or comments on this post.</p> <h3 id="problem-statement">Problem Statement</h3> <p>Monte Carlo Tree Search (MCTS) has emerged as a powerful algorithm for decision-making in complex, large-scale state spaces. Its success is evident in groundbreaking AI systems such as <a href="https://deepmind.google/research/breakthroughs/alphago/">AlphaGo</a>, <a href="https://deepmind.google/discover/blog/alphazero-shedding-new-light-on-chess-shogi-and-go/">AlphaZero</a>, and <a href="https://deepmind.google/technologies/alphafold/">AlphaFold</a>. With the recent advancements of large language models (LLMs) like <a href="https://openai.com/index/introducing-chatgpt-pro/">GPTs</a>, <a href="https://claude.ai/new">Claude</a>, and <a href="https://www.llama.com/">LLama</a> in the NLP community, there has been a growing interest in leveraging these models with MCTS frameworks.</p> <p>Recent research (<a href="https://arxiv.org/abs/2305.14078">Zhao et al. 2023</a>, <a href="https://arxiv.org/abs/2310.04406">Zhou et al. 2024</a> ) has explored the integration of LLMs into MCTS. It has been speculated that OpenAI’s <a href="https://openai.com/o1/">o1 model</a> employs MCTS, using the model’s confidence to guide the search and expand the solution space (<a href="https://arxiv.org/pdf/2411.14405v1">Zhao et al. 2024</a>). The core idea is to use the LLM’s world model to provide a prior belief of states, enabling more informed decision-making.</p> <p>Traditionally, MCTS relies on a rollout policy to estimate the value function, which guides the search towards relevant parts of the tree. However, this approach may be less effective in high-dimensional state spaces where accurate rollouts are challenging to obtain. While MCTS is primarily an on-policy algorithm, techniques such as importance sampling (IS) and doubly robust (DR) estimators—typically associated with off-policy evaluation—have not been widely applied to MCTS.</p> <p>Naturally, the question arises: Can we leverage the strengths of DR estimators to enhance value function estimation in MCTS?</p> <h3 id="why-doubly-robust-estimators">Why Doubly Robust Estimators?</h3> <p><a href="https://academic.oup.com/biomet/article-abstract/82/4/805/252258">Doubly robust (DR) estimators</a> have gained significant popularity in causal inference due to their unique properties. Originally developed for missing data problems, DR estimators have found widespread use in observational studies and clinical trials. The key advantage of DR estimators is their ability to provide unbiased estimates if either the outcome model or the propensity score model is correctly specified, offering a safeguard against model misspecification.</p> <p>In causal inference, DR estimators combine two approaches:</p> <ol> <li>An outcome regression model that predicts the outcome based on covariates and treatment.</li> <li>A propensity score model that estimates the probability of receiving treatment given covariates.</li> </ol> <p>This dual modeling approach provides a “double” chance of getting the correct estimate, hence the term “doubly robust.”</p> <p>The success of DR estimators in causal inference has inspired researchers to adapt these methods to the reinforcement learning (RL) domain. Two seminal papers have been instrumental in this transition:</p> <ol> <li> <p><a href="https://arxiv.org/abs/1511.03722">Jiang and Li (2016)</a> introduced the idea of using DR estimators for off-policy evaluation in RL. Their work demonstrated how DR estimators could be applied to estimate the value of a target policy using data collected from a different behavior policy. This approach showed promising results in reducing variance and bias compared to traditional importance sampling methods.</p> </li> <li> <p><a href="https://arxiv.org/abs/1604.00923">Thomas and Brunskill (2016)</a> further developed this concept by proposing a more data-efficient DR estimator for off-policy evaluation. Their method, known as the weighted doubly robust estimator, incorporated adaptive importance sampling techniques to improve estimation accuracy, especially in scenarios with limited data.</p> </li> </ol> <p>These pioneering works established DR estimators as a powerful tool in RL, particularly for off-policy evaluation. The success of DR estimators in this context stems from their ability to leverage both the direct method (using a model of the environment) and importance sampling, combining the strengths of both approaches while mitigating their individual weaknesses.</p> <h3 id="integrating-doubly-robust-estimators-with-mcts">Integrating Doubly Robust Estimators with MCTS</h3> <p>We hypothesize that the <strong>nice</strong> properties of DR estimators can be preserved when integrated with MCTS. In particular, we propose a hybrid estimator that combines the rollout policy in MCTS with a DR estimator to estimate the value function. This hybrid estimator leverages the strengths of both approaches:</p> <ol> <li>The rollout policy provides a quick estimate of the value function, guiding the search towards promising states.</li> <li>The DR estimator refines the value function estimate, reducing bias and variance in the estimation process.</li> </ol> <p>Below is my attempt to rigorously justify the integration of DR estimators with MCTS.</p> <h4 id="preliminaries">Preliminaries</h4> <p>Let \(\mathcal{S}\) be the state space, \(\mathcal{A}\) the action space, and \(\mathcal{H}\) the space of action-observation histories. We define the following:</p> <ul> <li>\(\pi_e: \mathcal{H} \times \mathcal{A} \rightarrow [0,1]\): the target policy (MCTS policy)</li> <li>\(\pi_b: \mathcal{H} \times \mathcal{A} \rightarrow [0,1]\): the behavior policy (LLM-based heuristic policy)</li> <li>\(Q: \mathcal{H} \times \mathcal{A} \rightarrow \mathbb{R}\): the true state-action value function</li> <li>\(\hat{Q}: \mathcal{H} \times \mathcal{A} \rightarrow \mathbb{R}\): the estimated state-action value function</li> <li>\(\hat{V}: \mathcal{H} \rightarrow \mathbb{R}\): the estimated state value function</li> <li>\(\gamma \in [0,1]\): the discount factor</li> </ul> <p>The target policy \(\pi_e\) is defined using a softmax over Q-values:</p> <p>\begin{equation} \pi_e(a|h) = \frac{\exp(Q(h,a)/\tau)}{\sum_{a’ \in \mathcal{A}} \exp(Q(h,a’)/\tau)} \end{equation}</p> <p>where \(\tau\) is a temperature parameter controlling the exploration-exploitation trade-off.</p> <h4 id="hybrid-estimator">Hybrid Estimator</h4> <p>We define our hybrid estimator as a combination of the standard MCTS rollout estimate and the DR estimate:</p> <p>\begin{equation} V_{\text{hybrid}}(h) = \beta V_{\text{MCTS}}(h) + (1-\beta) V_{DR}(h) \end{equation}</p> <p>where \(\beta \in [0,1]\) is a weighting parameter, \(V_{\text{MCTS}}(h)\) is the standard MCTS rollout estimate, and \(V_{DR}(h)\) is the DR estimate.</p> <h4 id="unbiasedness-of-the-hybrid-estimator">Unbiasedness of the hybrid estimator</h4> <p><strong>Theorem</strong>: The hybrid estimator is unbiased for estimating the value of the target policy \(\pi_e\) in the LLM-guided MCTS.</p> <p><em>Proof</em>: We know that \(V_{\text{MCTS}}(h)\) is unbiased due to the properties of Monte Carlo estimation. For \(V_{DR}(h)\), we can express it as:</p> <p>\begin{equation} V_{DR}(h) = \hat{V}(h) + \sum_{t=0}^{H-1} \gamma^t \rho_{1:t} (r_t + \gamma \hat{V}(h_{t+1}) - \hat{Q}(h_t, a_t)) \end{equation}</p> <p>where \(\rho_{1:t} = \prod_{k=1}^t \frac{\pi_e(a_k|h_k)}{\pi_b(a_k|h_k)}\) is the cumulative importance ratio.</p> <p>Taking the expectation with respect to the behavior policy \(\pi_b\):</p> \[\begin{align} \mathbb{E}_{\pi_b}[V_{DR}(h)] &amp;= \mathbb{E}_{\pi_b}[\hat{V}(h)] + \mathbb{E}_{\pi_b}\left[\sum_{t=0}^{H-1} \gamma^t \rho_{1:t} (r_t + \gamma \hat{V}(h_{t+1}) - \hat{Q}(h_t, a_t))\right] \\ &amp;= \hat{V}(h) + \sum_{t=0}^{H-1} \gamma^t \mathbb{E}_{\pi_b}[\rho_{1:t} (r_t + \gamma \hat{V}(h_{t+1}) - \hat{Q}(h_t, a_t))] \\ &amp;= \hat{V}(h) + \sum_{t=0}^{H-1} \gamma^t (Q(h_t, a_t) - \mathbb{E}_{\pi_e}[\hat{Q}(h_t, a_t)]) \\ &amp;= V(h) \end{align}\] <p>Therefore, both \(V_{\text{MCTS}}(h)\) and \(V_{DR}(h)\) are unbiased estimators of \(V(h)\). Since \(V_{\text{hybrid}}(h)\) is a linear combination of these unbiased estimators, it is also unbiased:</p> \[\begin{align} \mathbb{E}[V_{\text{hybrid}}(h)] &amp;= \mathbb{E}[\beta V_{\text{MCTS}}(h) + (1-\beta) V_{DR}(h)] \\ &amp;= \beta \mathbb{E}[V_{\text{MCTS}}(h)] + (1-\beta) \mathbb{E}[V_{DR}(h)] \\ &amp;= \beta V(h) + (1-\beta) V(h) \\ &amp;= V(h) \end{align}\] <p>Thus, the hybrid estimator remains unbiased in the LLM-guided MCTS context.</p> <h4 id="variance-reduction-of-the-hybrid-estimator">Variance Reduction of the hybrid estimator</h4> <p><strong>Theorem</strong>: Under certain conditions, the hybrid estimator has lower variance than the standard MCTS value estimator in the LLM-guided MCTS.</p> <p><em>Proof</em>: We aim to show that \(\text{Var}(V_{\text{hybrid}}(h)) \leq \text{Var}(V_{\text{MCTS}}(h))\) under certain conditions.</p> <p>The variance of the hybrid estimator is:</p> \[\begin{align} \text{Var}(V_{\text{hybrid}}(h)) &amp;= \text{Var}(\beta V_{\text{MCTS}}(h) + (1-\beta) V_{DR}(h)) \\ &amp;= \beta^2 \text{Var}(V_{\text{MCTS}}(h)) + (1-\beta)^2 \text{Var}(V_{DR}(h)) + 2\beta(1-\beta)\text{Cov}(V_{\text{MCTS}}(h), V_{DR}(h)) \end{align}\] <p>We can express \(V_{DR}(h)\) as \(V_{\text{MCTS}}(h) + \Delta(h)\), where \(\Delta(h) = \sum_{t=0}^{H-1} \gamma^t \rho_{1:t} (\hat{Q}(h_t, a_t) - r_t - \gamma \hat{V}(h_{t+1}))\).</p> <p>Substituting this into the variance equation: \(\begin{align} \text{Var}(V_{\text{hybrid}}(h)) &amp;= \beta^2 \text{Var}(V_{\text{MCTS}}(h)) + (1-\beta)^2 \text{Var}(V_{\text{MCTS}}(h) + \Delta(h)) \\ &amp;\quad + 2\beta(1-\beta)\text{Cov}(V_{\text{MCTS}}(h), V_{\text{MCTS}}(h) + \Delta(h)) \\ &amp;= \text{Var}(V_{\text{MCTS}}(h)) + (1-\beta)^2 \text{Var}(\Delta(h)) + 2(1-\beta)\text{Cov}(V_{\text{MCTS}}(h), \Delta(h)) \end{align}\)</p> <p>To show that \(\text{Var}(V_{\text{hybrid}}(h)) \leq \text{Var}(V_{\text{MCTS}}(h))\), it suffices to show that:</p> <p>\begin{equation} (1-\beta)^2 \text{Var}(\Delta(h)) + 2(1-\beta)\text{Cov}(V_{\text{MCTS}}(h), \Delta(h)) \leq 0 \end{equation}</p> <p>Examining \(\text{Cov}(V_{\text{MCTS}}(h), \Delta(h))\):</p> \[\begin{align} \text{Cov}(V_{\text{MCTS}}(h), \Delta(h)) &amp;= \mathbb{E}[V_{\text{MCTS}}(h)\Delta(h)] - \mathbb{E}[V_{\text{MCTS}}(h)]\mathbb{E}[\Delta(h)] \\ &amp;= \mathbb{E}[V_{\text{MCTS}}(h)\Delta(h)] \quad \text{(since $\mathbb{E}[\Delta(h)] = 0$ due to DR's unbiasedness)} \\ &amp;\approx -\mathbb{E}\left[\sum_{t=0}^{H-1} \gamma^{2t} \rho_{1:t}^2 (Q(h_t, a_t) - \hat{Q}(h_t, a_t))^2\right] \end{align}\] <p>The last approximation holds because the cross-terms tend to cancel out due to the Markov property.</p> <p>Observe that:</p> <p>\begin{equation} \text{Var}(\Delta(h)) = \mathbb{E}\left[\sum_{t=0}^{H-1} \gamma^{2t} \rho_{1:t}^2 (\hat{Q}(h_t, a_t) - r_t - \gamma \hat{V}(h_{t+1}))^2\right] \end{equation}</p> <p>Therefore:</p> <p>\begin{equation} (1-\beta)^2 \text{Var}(\Delta(h)) + 2(1-\beta)\text{Cov}(V_{\text{MCTS}}(h), \Delta(h)) \approx -(1-\beta)\mathbb{E}\left[\sum_{t=0}^{H-1} \gamma^{2t} \rho_{1:t}^2 (Q(h_t, a_t) - \hat{Q}(h_t, a_t))^2\right] \leq 0 \end{equation}</p> <p>This inequality holds as long as \(\hat{Q}\) is a reasonable approximation of \(Q\). In the context of LLM-guided MCTS, where we have estimates of \(Q(h,a)\) from both the tree search and the LLM policy, we can expect this condition to be satisfied.</p> <p>Thus, under these conditions, we have shown that \(\text{Var}(V_{\text{hybrid}}(h)) \leq \text{Var}(V_{\text{MCTS}}(h))\), demonstrating the variance reduction property of the hybrid estimator compared to the standard MCTS value estimator in the LLM-guided MCTS context.</p> <h3 id="advantages-of-the-hybrid-estimator">Advantages of the hybrid estimator</h3> <p>We speculate that the hybrid estimator offers several advantages over the standard MCTS value estimator:</p> <ol> <li><strong>Bias-Variance Trade-off</strong>: The hybrid estimator strikes a balance between the low bias of the MCTS rollout estimate and the low variance of the DR estimate, potentially leading to more stable and accurate value function estimates.</li> <li><strong>Data Efficiency</strong>: By leveraging the DR estimator, the hybrid estimator may require fewer samples to achieve the same level of accuracy as the MCTS rollout estimate, making it more data-efficient.</li> <li><strong>Robustness</strong>: The hybrid estimator is robust to model misspecification. In particular, if the rollout policy is inaccurate, the DR estimator can correct for this bias via importance sampling and the outcome model (value function estimate).</li> </ol> <h3 id="conclusion">Conclusion</h3> <p>In this post, we have explored the theoretical justification for integrating doubly robust estimators with MCTS. We have proposed a hybrid estimator that combines the strengths of MCTS rollouts and DR estimators to enhance value function estimation in the LLM-guided MCTS framework. Our analysis suggests that the hybrid estimator is unbiased and can potentially reduce variance compared to the standard MCTS value estimator. We believe that this integration could lead to more robust, data-efficient decision-making in complex state spaces, particularly when using large language models like GPTs to guide the search. We are excited about the potential of this approach and look forward to empirical validation in future work in the context of LLM-guided MCTS.</p>]]></content><author><name></name></author><category term="research"/><category term="work-in-progress"/><category term="RL"/><category term="causality"/><category term="comments"/><summary type="html"><![CDATA[A blog post on integrating doubly robust estimators with MCTS]]></summary></entry><entry><title type="html">The bitter lessons I learnt from my first PhD project</title><link href="https://manqingliu.github.io/blog/2025/my-first-blog/" rel="alternate" type="text/html" title="The bitter lessons I learnt from my first PhD project"/><published>2025-01-03T00:00:00+00:00</published><updated>2025-01-03T00:00:00+00:00</updated><id>https://manqingliu.github.io/blog/2025/my-first-blog</id><content type="html" xml:base="https://manqingliu.github.io/blog/2025/my-first-blog/"><![CDATA[<p>I recently completed my first PhD <a href="https://arxiv.org/abs/2410.10044">project</a>, and I must say it was a bittersweet experience. I learned a lot, but I also made plenty of mistakes. In this post, I’ll share the bitter lessons I picked up along the way.</p> <p>A quick disclaimer: I’m no expert in this field, so definitely read it with a grain of salt. I’m sharing my experience in the hope that it’ll help others avoid the pitfalls I encountered, especially those who, like me, don’t come from a traditional computer science background.</p> <h3 id="lesson-1-start-with-an-existing-repository-if-you-can-find-one-and-build-on-it">Lesson 1: Start with an existing repository if you can find one and build on it</h3> <p>When I kicked off my first PhD project, I decided to build everything from scratch. I thought it would give me a deeper understanding and more control. At the time, I was new to Python and had a deep-rooted fear of using or changing other people’s code.</p> <p>This turned out to be a massive mistake that cost me valuable time and energy. Looking back, I should have searched for existing repositories or codebases related to my research topic.</p> <p>Starting from an existing codebase would have saved me months of work setting up basic infrastructures and implementing standard algorithms. I could have learned best practices and coding standards specific to my field by examining and building upon existing code. Instead of reinventing the wheel, I could have devoted more time to developing and implementing my unique ideas.</p> <p>Of course, this doesn’t mean you should never build from scratch. There are times when it’s necessary or beneficial. But for most PhD projects, especially in the early stages, leveraging existing work can significantly accelerate your progress and allow you to focus on your unique contributions.</p> <h3 id="lesson-2-check-public-datasets-first-before-simulating-your-own-data">Lesson 2: Check public datasets first before simulating your own data</h3> <p>I was eager to create a custom dataset that perfectly matched my research needs. I spent months developing complex simulations to generate synthetic data. However, this decision came with several unforeseen challenges.</p> <p>Creating and validating a synthetic dataset took far longer than anticipated, especially when trying to create a dataset as close to real-world as possible. In the context of causal inference, simulating a data-generating process that considers all possible interactions among confounders, treatment, and outcome is incredibly challenging and time-consuming.</p> <p>Later, I discovered several public datasets with various numbers of covariates and ground truth causal effects that could have served my needs with minimal modifications. I had reinvented the wheel unnecessarily.</p> <p>In hindsight, I should have first thoroughly investigated existing public datasets. Many fields have well-established, high-quality public datasets that are regularly used and validated by the research community. Using these allows you to start your actual research faster and enables direct comparison of your methods with existing work.</p> <p>That’s not to say creating your own dataset is always wrong, especially for initial small-scale hypothesis testing. But consider public datasets when scaling up your research.</p> <h3 id="lesson-3-dont-be-over-reliant-on-generative-ai-for-debugging">Lesson 3: Don’t be over-reliant on generative AI for debugging</h3> <p>Pursuing a technical PhD in GenAI is like opening Pandora’s box. Initially, my workflow was a frustrating cycle: a terrifying red error message would pop up, I’d frantically consult ChatGPT, receive a solution I barely comprehended, attempt to implement it, only to face another error message. This cycle would repeat until, miraculously, the solution worked - leaving me none the wiser about why.</p> <p>While generative AI can be a powerful debugging tool, it’s a double-edged sword, especially for beginners. It’s dangerously easy to fall into the trap of mental laziness, allowing the AI to do all the thinking. This approach can lead to either getting the right solution or merely suppressing the error message while the underlying problem persists.</p> <p>I confess I still fall into this trap occasio nally. However, I’m striving to be more mindful of AI-provided solutions. My new approach is to always attempt solving the bug myself first, understanding the root cause, before considering the AI’s suggestion.</p> <p>In AI research, understanding the ‘why’ behind a solution is often more crucial than the solution itself. Let’s use AI as a tool to augment our problem-solving skills, not replace them.</p> <h3 id="lesson-4-dont-be-afraid-to-ask-for-help-even-if-you-think-you-should-know-the-answer">Lesson 4: Don’t be afraid to ask for help, even if you think you should know the answer</h3> <p>Just as I was tempted to let AI do all the thinking, I was also hesitant to seek human help. I thought I should know the answers, or I assumed my code was correct simply because it ran without throwing errors. This mindset, I’ve learned, can be incredibly dangerous in research.</p> <p>For the second part of my first project, which was a continuation of a <a href="https://arxiv.org/abs/2205.09824">paper</a> by former lab mates. One of the leading authors is <a href="https://davidbellamy.github.io/">David R. Bellamy</a> who is a brilliant researcher and a great mentor. I finally mustered the courage to reach out for guidance. This decision proved invaluable. When David reviewed my code, we uncovered a silent error - one that didn’t trigger any warnings but significantly impacted the results. Something as seemingly minor as the difference between <code class="language-plaintext highlighter-rouge">tensor.view()</code> and <code class="language-plaintext highlighter-rouge">tensor.permute()</code> led to substantial discrepancies in the output.</p> <p>The process of code review turned into much more than just error-hunting. It became an impromptu masterclass in coding standards, best practices, debugging techniques, and how to write clean, readable code. David’s willingness to help and share knowledge reminded me of a fundamental truth in academia: we’re all here to learn and grow together.</p> <h3 id="lesson-5-keep-a-daily-log-of-your-experiments">Lesson 5: Keep a daily log of your experiments</h3> <p>This lesson was profoundly influenced by <a href="http://joschu.net/">John Schulman</a>’s insightful <a href="http://joschu.net/blog/opinionated-guide-ml-research.html">blog post</a> on conducting ML research. I wish I’d stumbled upon it earlier in my PhD journey!</p> <p>From the outset, I diligently used <a href="https://wandb.ai/site/">Weights &amp; Biases</a> to log my experiments. However, I overlooked the crucial practice of maintaining a daily log. This oversight made tracking my progress a Herculean task. My workflow devolved into a haphazard cycle of tweaking parameters and re-running experiments without clear direction.</p> <p>One of the most insidious traps in my PhD journey was the illusion of time well spent. Sure, I was constantly running experiments, but without a daily log detailing my plans, code changes, results, and reflections, I was essentially stumbling around in the dark.</p> <p>So, let me implore you: keep a daily log of your experiments! Include your plan for the day, specific changes made to your code, results corresponding to each change, reflections and insights gained, and next steps based on these outcomes.</p> <p>This practice isn’t just about record-keeping; it’s about cultivating a mindful approach to your research. It will help you track progress, understand patterns, make informed decisions, avoid repeating unsuccessful approaches, and reflect on your growth as a researcher.</p> <h3 id="lesson-6-sometimes-a-bottom-up-approach-is-better-than-a-top-down-approach">Lesson 6: Sometimes a bottom up approach is better than a top-down approach</h3> <p>My first project aimed to leverage transformer models for causal effect estimation. Excited by the potential, I dove headfirst into using PyTorch’s ready-made <code class="language-plaintext highlighter-rouge">Transformer</code> model. Our idea seemed straightforward, but reality hit us like a ton of bricks.</p> <p>Our grand transformer model refused to converge, the loss stubbornly refused to decrease, and its performance lagged behind “simple” models like random forests or Multilayer Perceptron (MLP). I spent months tweaking the model and adjusting parameters, but the results remained disappointing.</p> <p>The breakthrough came when I decided to take a step back and adopt a bottom-up approach. I started by implementing a simple MLP model, gradually incorporating transformer components like self-attention layers. This incremental approach allowed me to understand the transformer’s inner workings, identify bottlenecks, and make targeted improvements.</p> <p>This experience taught me a valuable lesson: sometimes, a bottom-up approach is more effective than a top-down approach. By starting with simple, interpretable models and gradually incorporating complex components, you gain a deeper understanding of your model’s behavior and can make more informed decisions.</p> <h3 id="lesson-7-idea-is-cheap-the-devil-is-in-the-details">Lesson 7: Idea is cheap, the devil is in the details</h3> <p>Having an “original” idea is relatively easy. The real challenge lies in implementation and execution. You’ll never know if your intuition is correct until you’ve rigorously tested it, and often the result isn’t what you expected. This is what makes research both exciting and frustrating.</p> <p>In my experience, the gap between a promising idea and successful implementation is filled with countless hours of debugging, parameter tuning, and sometimes rethinking the entire approach. It’s in these details where true innovation often happens.</p> <p>I’ve learned that persistence is key, but so is flexibility. Sometimes, pushing forward means being willing to pivot your approach while still pursuing the core of your original idea. It’s about finding a balance between stubbornness in your vision and adaptability in your methods.</p> <p>Documentation and collaboration become your best friends in this process. Keep detailed notes and don’t be afraid to discuss your challenges with peers or mentors.</p> <p>Many groundbreaking discoveries came not from the initial idea, but from the persistent work that followed. The difference between a good idea and a great contribution often lies in your ability to tackle the nitty-gritty details with creativity and perseverance.</p> <p>So, the next time you’re stuck in the quagmire of implementation details, this might be where the real magic of research happens. Embrace the challenge, stay curious, and keep pushing forward. Your breakthrough might be just around the corner, hidden in the details you’re currently wrestling with.</p>]]></content><author><name></name></author><category term="blog-posts"/><category term="reflections"/><category term="comments"/><summary type="html"><![CDATA[A post about the lessons learnt from working on my first PhD project.]]></summary></entry></feed>