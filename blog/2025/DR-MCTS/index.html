<!DOCTYPE html> <html lang="en"> <head> <meta http-equiv="Content-Type" content="text/html; charset=UTF-8"> <meta charset="utf-8"> <meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no"> <meta http-equiv="X-UA-Compatible" content="IE=edge"> <title> On integrating doubly robust estimators with MCTS | Manqing Liu </title> <meta name="author" content="Manqing Liu"> <meta name="description" content="A blog post on integrating doubly robust estimators with MCTS"> <meta name="keywords" content="jekyll, jekyll-theme, academic-website, portfolio-website"> <link rel="stylesheet" href="/assets/css/bootstrap.min.css?a4b3f509e79c54a512b890d73235ef04"> <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/css/mdb.min.css" integrity="sha256-jpjYvU3G3N6nrrBwXJoVEYI/0zw8htfFnhT9ljN3JJw=" crossorigin="anonymous"> <link defer rel="stylesheet" href="/assets/css/academicons.min.css?f0b7046b84e425c55f3463ac249818f5"> <link defer rel="stylesheet" href="/assets/css/scholar-icons.css?62b2ac103a88034e6882a5be5f3e2772"> <link defer rel="stylesheet" type="text/css" href="https://fonts.googleapis.com/css?family=Roboto:300,400,500,700|Roboto+Slab:100,300,400,500,700|Material+Icons&amp;display=swap"> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-github.css?591dab5a4e56573bf4ef7fd332894c99" media="" id="highlight_theme_light"> <link rel="shortcut icon" href="/assets/img/polar-bear.png?ade01a7410afd14d3d0021101e36f068"> <link rel="stylesheet" href="/assets/css/main.css?d41d8cd98f00b204e9800998ecf8427e"> <link rel="canonical" href="https://manqingliu.github.io/blog/2025/DR-MCTS/"> <script src="/assets/js/theme.js?9a0c749ec5240d9cda97bc72359a72c0"></script> <link defer rel="stylesheet" href="/assets/css/jekyll-pygments-themes-native.css?5847e5ed4a4568527aa6cfab446049ca" media="none" id="highlight_theme_dark"> <script>
    initTheme();
  </script> </head> <body class="fixed-top-nav "> <header> <nav id="navbar" class="navbar navbar-light navbar-expand-sm fixed-top" role="navigation"> <div class="container"> <a class="navbar-brand title font-weight-lighter" href="/"> <span class="font-weight-bold">Manqing</span> Liu </a> <button class="navbar-toggler collapsed ml-auto" type="button" data-toggle="collapse" data-target="#navbarNav" aria-controls="navbarNav" aria-expanded="false" aria-label="Toggle navigation"> <span class="sr-only">Toggle navigation</span> <span class="icon-bar top-bar"></span> <span class="icon-bar middle-bar"></span> <span class="icon-bar bottom-bar"></span> </button> <div class="collapse navbar-collapse text-right" id="navbarNav"> <ul class="navbar-nav ml-auto flex-nowrap"> <li class="nav-item "> <a class="nav-link" href="/">about </a> </li> <li class="nav-item active"> <a class="nav-link" href="/blog/">blog </a> </li> <li class="nav-item "> <a class="nav-link" href="/publications/">publications </a> </li> <li class="nav-item "> <a class="nav-link" href="/repositories/">repositories </a> </li> <li class="nav-item "> <a class="nav-link" href="/cv/">cv </a> </li> <li class="nav-item dropdown "> <a class="nav-link dropdown-toggle" href="#" id="navbarDropdown" role="button" data-toggle="dropdown" aria-haspopup="true" aria-expanded="false">submenus </a> <div class="dropdown-menu dropdown-menu-right" aria-labelledby="navbarDropdown"> <a class="dropdown-item " href="/publications/">publications</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/projects/">projects</a> <div class="dropdown-divider"></div> <a class="dropdown-item " href="/blog/">blog</a> </div> </li> <li class="nav-item"> <button id="search-toggle" title="Search" onclick="openSearchModal()"> <span class="nav-link">ctrl k <i class="ti ti-search"></i></span> </button> </li> <li class="toggle-container"> <button id="light-toggle" title="Change theme"> <i class="ti ti-sun-moon" id="light-toggle-system"></i> <i class="ti ti-moon-filled" id="light-toggle-dark"></i> <i class="ti ti-sun-filled" id="light-toggle-light"></i> </button> </li> </ul> </div> </div> </nav> <progress id="progress" value="0"> <div class="progress-container"> <span class="progress-bar"></span> </div> </progress> </header> <div class="container mt-5" role="main"> <div class="post"> <header class="post-header"> <h1 class="post-title">On integrating doubly robust estimators with MCTS</h1> <p class="post-meta"> Created in January 08, 2025 </p> <p class="post-tags"> <a href="/blog/2025"> <i class="fa-solid fa-calendar fa-sm"></i> 2025 </a>   ·   <a href="/blog/tag/rl"> <i class="fa-solid fa-hashtag fa-sm"></i> RL</a>   <a href="/blog/tag/causality"> <i class="fa-solid fa-hashtag fa-sm"></i> causality</a>   <a href="/blog/tag/comments"> <i class="fa-solid fa-hashtag fa-sm"></i> comments</a>   ·   <a href="/blog/category/research"> <i class="fa-solid fa-tag fa-sm"></i> research</a>   <a href="/blog/category/work-in-progress"> <i class="fa-solid fa-tag fa-sm"></i> work-in-progress</a> </p> </header> <article class="post-content"> <div id="table-of-contents"> <ul id="toc" class="section-nav"> <li class="toc-entry toc-h3"><a href="#problem-statement">Problem Statement</a></li> <li class="toc-entry toc-h3"><a href="#why-doubly-robust-estimators">Why Doubly Robust Estimators?</a></li> <li class="toc-entry toc-h3"> <a href="#integrating-doubly-robust-estimators-with-mcts">Integrating Doubly Robust Estimators with MCTS</a> <ul> <li class="toc-entry toc-h4"><a href="#preliminaries">Preliminaries</a></li> <li class="toc-entry toc-h4"><a href="#hybrid-estimator">Hybrid Estimator</a></li> <li class="toc-entry toc-h4"><a href="#proof-of-unbiasedness-of-the-hybrid-estimator">Proof of unbiasedness of the hybrid estimator</a></li> </ul> </li> </ul> </div> <hr> <div id="markdown-content"> <p>In this post, I will explore the <strong>theoretical</strong> justification for integrating doubly robust estimators into Monte Carlo Tree Search (MCTS). I welcome any feedback or comments on this post.</p> <h3 id="problem-statement">Problem Statement</h3> <p>Monte Carlo Tree Search (MCTS) has emerged as a powerful algorithm for decision-making in complex, large-scale state spaces. Its success is evident in groundbreaking AI systems such as <a href="https://deepmind.google/research/breakthroughs/alphago/" rel="external nofollow noopener" target="_blank">AlphaGo</a>, <a href="https://deepmind.google/discover/blog/alphazero-shedding-new-light-on-chess-shogi-and-go/" rel="external nofollow noopener" target="_blank">AlphaZero</a>, and <a href="https://deepmind.google/technologies/alphafold/" rel="external nofollow noopener" target="_blank">AlphaFold</a>. With the recent advancements of large language models (LLMs) like <a href="https://openai.com/index/introducing-chatgpt-pro/" rel="external nofollow noopener" target="_blank">GPTs</a>, <a href="https://claude.ai/new" rel="external nofollow noopener" target="_blank">Claude</a>, and <a href="https://www.llama.com/" rel="external nofollow noopener" target="_blank">LLama</a> in the NLP community, there has been a growing interest in leveraging these models with MCTS frameworks.</p> <p>Recent research (<a href="https://arxiv.org/abs/2305.14078" rel="external nofollow noopener" target="_blank">Zhao et al. 2023</a>, <a href="https://arxiv.org/abs/2310.04406" rel="external nofollow noopener" target="_blank">Zhou et al. 2024</a> ) has explored the integration of LLMs into MCTS. It has been speculated that OpenAI’s <a href="https://openai.com/o1/" rel="external nofollow noopener" target="_blank">o1 model</a> employs MCTS, using the model’s confidence to guide the search and expand the solution space (<a href="https://arxiv.org/pdf/2411.14405v1" rel="external nofollow noopener" target="_blank">Zhao et al. 2024</a>). The core idea is to use the LLM’s world model to provide a prior belief of states, enabling more informed decision-making.</p> <p>Traditionally, MCTS relies on a rollout policy to estimate the value function, which guides the search towards relevant parts of the tree. However, this approach may be less effective in high-dimensional state spaces where accurate rollouts are challenging to obtain. While MCTS is primarily an on-policy algorithm, techniques such as importance sampling (IS) and doubly robust (DR) estimators—typically associated with off-policy evaluation—have not been widely applied to MCTS.</p> <p>Naturally, the question arises: Can we leverage the strengths of DR estimators to enhance value function estimation in MCTS?</p> <h3 id="why-doubly-robust-estimators">Why Doubly Robust Estimators?</h3> <p><a href="https://academic.oup.com/biomet/article-abstract/82/4/805/252258" rel="external nofollow noopener" target="_blank">Doubly robust (DR) estimators</a> have gained significant popularity in causal inference due to their unique properties. Originally developed for missing data problems, DR estimators have found widespread use in observational studies and clinical trials. The key advantage of DR estimators is their ability to provide unbiased estimates if either the outcome model or the propensity score model is correctly specified, offering a safeguard against model misspecification.</p> <p>In causal inference, DR estimators combine two approaches:</p> <ol> <li>An outcome regression model that predicts the outcome based on covariates and treatment.</li> <li>A propensity score model that estimates the probability of receiving treatment given covariates.</li> </ol> <p>This dual modeling approach provides a “double” chance of getting the correct estimate, hence the term “doubly robust.”</p> <p>The success of DR estimators in causal inference has inspired researchers to adapt these methods to the reinforcement learning (RL) domain. Two seminal papers have been instrumental in this transition:</p> <ol> <li> <p><a href="https://arxiv.org/abs/1511.03722" rel="external nofollow noopener" target="_blank">Jiang and Li (2016)</a> introduced the idea of using DR estimators for off-policy evaluation in RL. Their work demonstrated how DR estimators could be applied to estimate the value of a target policy using data collected from a different behavior policy. This approach showed promising results in reducing variance and bias compared to traditional importance sampling methods.</p> </li> <li> <p><a href="https://arxiv.org/abs/1604.00923" rel="external nofollow noopener" target="_blank">Thomas and Brunskill (2016)</a> further developed this concept by proposing a more data-efficient DR estimator for off-policy evaluation. Their method, known as the weighted doubly robust estimator, incorporated adaptive importance sampling techniques to improve estimation accuracy, especially in scenarios with limited data.</p> </li> </ol> <p>These pioneering works established DR estimators as a powerful tool in RL, particularly for off-policy evaluation. The success of DR estimators in this context stems from their ability to leverage both the direct method (using a model of the environment) and importance sampling, combining the strengths of both approaches while mitigating their individual weaknesses.</p> <h3 id="integrating-doubly-robust-estimators-with-mcts">Integrating Doubly Robust Estimators with MCTS</h3> <p>We hypothesize that the <strong>nice</strong> properties of DR estimators can be preserved when integrated with MCTS. In particular, we propose a hybrid estimator that combines the rollout policy in MCTS with a DR estimator to estimate the value function. This hybrid estimator leverages the strengths of both approaches:</p> <ol> <li>The rollout policy provides a quick estimate of the value function, guiding the search towards promising states.</li> <li>The DR estimator refines the value function estimate, reducing bias and variance in the estimation process.</li> </ol> <p>Below is my attempt to rigorously justify the integration of DR estimators with MCTS.</p> <h4 id="preliminaries">Preliminaries</h4> <p>Let \(\mathcal{S}\) be the state space, \(\mathcal{A}\) the action space, and \(\mathcal{H}\) the space of action-observation histories. We define the following:</p> <ul> <li>\(\pi_e: \mathcal{H} \times \mathcal{A} \rightarrow [0,1]\): the target policy (MCTS policy)</li> <li>\(\pi_b: \mathcal{H} \times \mathcal{A} \rightarrow [0,1]\): the behavior policy (LLM-based heuristic policy)</li> <li>\(Q: \mathcal{H} \times \mathcal{A} \rightarrow \mathbb{R}\): the true state-action value function</li> <li>\(\hat{Q}: \mathcal{H} \times \mathcal{A} \rightarrow \mathbb{R}\): the estimated state-action value function</li> <li>\(\hat{V}: \mathcal{H} \rightarrow \mathbb{R}\): the estimated state value function</li> <li>\(\gamma \in [0,1]\): the discount factor</li> </ul> <p>The target policy \(\pi_e\) is defined using a softmax over Q-values:</p> <p>\begin{equation} \pi_e(a|h) = \frac{\exp(Q(h,a)/\tau)}{\sum_{a’ \in \mathcal{A}} \exp(Q(h,a’)/\tau)} \end{equation}</p> <p>where \(\tau\) is a temperature parameter controlling the exploration-exploitation trade-off.</p> <h4 id="hybrid-estimator">Hybrid Estimator</h4> <p>We define our hybrid estimator as a combination of the standard MCTS rollout estimate and the DR estimate:</p> <p>\begin{equation} V_{\text{hybrid}}(h) = \beta V_{\text{MCTS}}(h) + (1-\beta) V_{DR}(h) \end{equation}</p> <p>where \(\beta \in [0,1]\) is a weighting parameter, \(V_{\text{MCTS}}(h)\) is the standard MCTS rollout estimate, and \(V_{DR}(h)\) is the DR estimate.</p> <h4 id="proof-of-unbiasedness-of-the-hybrid-estimator">Proof of unbiasedness of the hybrid estimator</h4> \[\theorem{Unbiasedness of Hybrid Estimator: The hybrid estimator is unbiased for estimating the value of the target policy $\pi_e$ in the LLM-guided MCTS.}\] \[\proof{We know that $V_{\text{MCTS}}(h)$ is unbiased due to the properties of Monte Carlo estimation. For $V_{DR}(h)$, we can express it as: \begin{align} V_{DR}(h) = \hat{V}(h) + \sum_{t=0}^{H-1} \gamma^t \rho_{1:t} (r_t + \gamma \hat{V}(h_{t+1}) - \hat{Q}(h_t, a_t)) \end{align} where $\rho_{1:t} = \prod_{k=1}^t \frac{\pi_e(a_k|h_k)}{\pi_b(a_k|h_k)}$ is the cumulative importance ratio. Taking the expectation with respect to the behavior policy $\pi_b$: \begin{align} \mathbb{E}_{\pi_b}[V_{DR}(h)] &amp;= \mathbb{E}_{\pi_b}[\hat{V}(h)] + \mathbb{E}_{\pi_b}\left[\sum_{t=0}^{H-1} \gamma^t \rho_{1:t} (r_t + \gamma \hat{V}(h_{t+1}) - \hat{Q}(h_t, a_t))\right] \\ &amp;= \hat{V}(h) + \sum_{t=0}^{H-1} \gamma^t \mathbb{E}_{\pi_b}[\rho_{1:t} (r_t + \gamma \hat{V}(h_{t+1}) - \hat{Q}(h_t, a_t))] \\ &amp;= \hat{V}(h) + \sum_{t=0}^{H-1} \gamma^t (Q(h_t, a_t) - \mathbb{E}_{\pi_e}[\hat{Q}(h_t, a_t)]) \\ &amp;= V(h) \end{align} Therefore, both $V_{\text{MCTS}}(h)$ and $V_{DR}(h)$ are unbiased estimators of $V(h)$. Since $V_{\text{hybrid}}(h)$ is a linear combination of these unbiased estimators, it is also unbiased: \begin{align} \mathbb{E}[V_{\text{hybrid}}(h)] &amp;= \mathbb{E}[\beta V_{\text{MCTS}}(h) + (1-\beta) V_{DR}(h)] \\ &amp;= \beta \mathbb{E}[V_{\text{MCTS}}(h)] + (1-\beta) \mathbb{E}[V_{DR}(h)] \\ &amp;= \beta V(h) + (1-\beta) V(h) \\ &amp;= V(h) \end{align} Thus, the hybrid estimator remains unbiased in the LLM-guided MCTS context.}\] </div> </article> <div id="giscus_thread" style="max-width: 930px; margin: 0 auto;"> <script>
      let giscusTheme = determineComputedTheme();
      let giscusAttributes = {
        src: 'https://giscus.app/client.js',
        'data-repo': 'ManqingLiu/ManqingLiu.github.io',
        'data-repo-id': 'R_kgDONkuz4g',
        'data-category': 'General',
        'data-category-id': 'DIC_kwDONkuz4s4ClvnY',
        'data-mapping': 'title',
        'data-strict': '0',
        'data-reactions-enabled': '1',
        'data-emit-metadata': '0',
        'data-input-position': 'bottom',
        'data-theme': giscusTheme,
        'data-lang': 'en',
        crossorigin: 'anonymous',
        async: '',
      };

      let giscusScript = document.createElement('script');
      Object.entries(giscusAttributes).forEach(([key, value]) => giscusScript.setAttribute(key, value));
      document.getElementById('giscus_thread').appendChild(giscusScript);
    </script> <noscript>Please enable JavaScript to view the <a href="http://giscus.app/?ref_noscript" rel="external nofollow noopener" target="_blank">comments powered by giscus.</a> </noscript> </div> </div> </div> <footer class="fixed-bottom" role="contentinfo"> <div class="container mt-0"> © Copyright 2025 Manqing Liu. Powered by <a href="https://jekyllrb.com/" target="_blank" rel="external nofollow noopener">Jekyll</a> with <a href="https://github.com/alshedivat/al-folio" rel="external nofollow noopener" target="_blank">al-folio</a> theme. Hosted by <a href="https://pages.github.com/" target="_blank" rel="external nofollow noopener">GitHub Pages</a>. Photos from <a href="https://unsplash.com" target="_blank" rel="external nofollow noopener">Unsplash</a>. </div> </footer> <script src="https://cdn.jsdelivr.net/npm/jquery@3.6.0/dist/jquery.min.js" integrity="sha256-/xUj+3OJU5yExlq6GSYGSHk7tPXikynS7ogEvDej/m4=" crossorigin="anonymous"></script> <script src="/assets/js/bootstrap.bundle.min.js"></script> <script src="https://cdn.jsdelivr.net/npm/mdbootstrap@4.20.0/js/mdb.min.js" integrity="sha256-NdbiivsvWt7VYCt6hYNT3h/th9vSTL4EDWeGs5SN3DA=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/masonry-layout@4.2.2/dist/masonry.pkgd.min.js" integrity="sha256-Nn1q/fx0H7SNLZMQ5Hw5JLaTRZp0yILA/FRexe19VdI=" crossorigin="anonymous"></script> <script defer src="https://cdn.jsdelivr.net/npm/imagesloaded@5.0.0/imagesloaded.pkgd.min.js" integrity="sha256-htrLFfZJ6v5udOG+3kNLINIKh2gvoKqwEhHYfTTMICc=" crossorigin="anonymous"></script> <script defer src="/assets/js/masonry.js?a0db7e5d5c70cc3252b3138b0c91dcaf" type="text/javascript"></script> <script defer src="https://cdn.jsdelivr.net/npm/medium-zoom@1.1.0/dist/medium-zoom.min.js" integrity="sha256-ZgMyDAIYDYGxbcpJcfUnYwNevG/xi9OHKaR/8GK+jWc=" crossorigin="anonymous"></script> <script defer src="/assets/js/zoom.js?85ddb88934d28b74e78031fd54cf8308"></script> <script src="/assets/js/no_defer.js?2781658a0a2b13ed609542042a859126"></script> <script defer src="/assets/js/common.js?e0514a05c5c95ac1a93a8dfd5249b92e"></script> <script defer src="/assets/js/copy_code.js?12775fdf7f95e901d7119054556e495f" type="text/javascript"></script> <script defer src="/assets/js/jupyter_new_tab.js?d9f17b6adc2311cbabd747f4538bb15f"></script> <script async src="https://d1bxh8uas1mnw7.cloudfront.net/assets/embed.js"></script> <script async src="https://badge.dimensions.ai/badge.js"></script> <script defer type="text/javascript" id="MathJax-script" src="https://cdn.jsdelivr.net/npm/mathjax@3.2.2/es5/tex-mml-chtml.js" integrity="sha256-MASABpB4tYktI2Oitl4t+78w/lyA+D7b/s9GEP0JOGI=" crossorigin="anonymous"></script> <script src="/assets/js/mathjax-setup.js?2456d95ccc88b061661f1496c7db3955"></script> <script defer src="https://cdnjs.cloudflare.com/polyfill/v3/polyfill.min.js?features=es6" crossorigin="anonymous"></script> <script defer src="/assets/js/progress-bar.js?2f30e0e6801ea8f5036fa66e1ab0a71a" type="text/javascript"></script> <script src="/assets/js/vanilla-back-to-top.min.js?f40d453793ff4f64e238e420181a1d17"></script> <script>
    addBackToTop();
  </script> <script type="module" src="/assets/js/search/ninja-keys.min.js?a3446f084dcaecc5f75aa1757d087dcf"></script> <ninja-keys hidebreadcrumbs noautoloadmdicons placeholder="Type to start searching"></ninja-keys> <script src="/assets/js/search-setup.js?6c304f7b1992d4b60f7a07956e52f04a"></script> <script src="/assets/js/search-data.js"></script> <script src="/assets/js/shortcut-key.js?6f508d74becd347268a7f822bca7309d"></script> </body> </html>