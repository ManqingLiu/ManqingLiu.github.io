---
layout: post
title: On integrating doubly robust estimators with MCTS 
date: 2025-01-08
description: A blog post on integrating doubly robust estimators with MCTS 
tags: RL causality comments
categories: research work-in-progress
giscus_comments: true
related_posts: false
toc:
  beginning: true
wip: true
---

In this post, I will explore the **theoretical** justification for integrating doubly robust estimators into Monte Carlo
Tree Search (MCTS). 
I welcome any feedback or comments on this post.

### Problem Statement

Monte Carlo Tree Search (MCTS) has emerged as a powerful algorithm for decision-making in complex, large-scale state spaces. 
Its success is evident in groundbreaking AI systems such as
[AlphaGo](https://deepmind.google/research/breakthroughs/alphago/), [AlphaZero](https://deepmind.google/discover/blog/alphazero-shedding-new-light-on-chess-shogi-and-go/),
and [AlphaFold](https://deepmind.google/technologies/alphafold/). With the recent advancements of large language models (LLMs)
like [GPTs](https://openai.com/index/introducing-chatgpt-pro/), [Claude](https://claude.ai/new), and [LLama](https://www.llama.com/) in the NLP community,
there has been a growing interest in leveraging these models with MCTS frameworks. 

Recent research ([Zhao et al. 2023](https://arxiv.org/abs/2305.14078), [Zhou et al. 2024](https://arxiv.org/abs/2310.04406)
) has explored the integration of LLMs into MCTS. It has been speculated that OpenAI's [o1 model](https://openai.com/o1/) employs MCTS, using the model's confidence to guide the search
and expand the solution space ([Zhao et al. 2024](https://arxiv.org/pdf/2411.14405v1)). The core idea is to use the LLM's world model to provide a prior belief of states, enabling more informed decision-making.

Traditionally, MCTS relies on a rollout policy to estimate the value function, which guides the search towards relevant 
parts of the tree. However, this approach may be less effective in high-dimensional state spaces where accurate rollouts 
are challenging to obtain. While MCTS is primarily an on-policy algorithm, techniques such as importance sampling (IS) 
and doubly robust (DR) estimators—typically associated with off-policy evaluation—have not been widely applied to MCTS.

Naturally, the question arises: Can we leverage the strengths of DR estimators to enhance value function estimation in MCTS?

### Why Doubly Robust Estimators?

[Doubly robust (DR) estimators](https://academic.oup.com/biomet/article-abstract/82/4/805/252258) have 
gained significant popularity in causal inference due to their unique properties. 
Originally developed for missing data problems, DR estimators have found widespread use in observational studies and 
clinical trials. The key advantage of DR estimators is their ability to provide unbiased estimates if either the 
outcome model or the propensity score model is correctly specified, offering a safeguard against model misspecification.

In causal inference, DR estimators combine two approaches:
1. An outcome regression model that predicts the outcome based on covariates and treatment.
2. A propensity score model that estimates the probability of receiving treatment given covariates.

This dual modeling approach provides a "double" chance of getting the correct estimate, hence the term "doubly robust."

The success of DR estimators in causal inference has inspired researchers to adapt these methods to the 
reinforcement learning (RL) domain. Two seminal papers have been instrumental in this transition:

1. [Jiang and Li (2016)](https://arxiv.org/abs/1511.03722) introduced the idea of using DR estimators for off-policy evaluation in RL. 
Their work demonstrated how DR estimators could be applied to estimate the value of a target policy using 
data collected from a different behavior policy. This approach showed promising results in reducing variance and 
bias compared to traditional importance sampling methods.

2. [Thomas and Brunskill (2016)](https://arxiv.org/abs/1604.00923) further developed this concept by proposing a more data-efficient DR estimator for 
off-policy evaluation. Their method, known as the weighted doubly robust estimator, incorporated adaptive importance 
sampling techniques to improve estimation accuracy, especially in scenarios with limited data.

These pioneering works established DR estimators as a powerful tool in RL, particularly for off-policy evaluation. 
The success of DR estimators in this context stems from their ability to leverage both the direct method 
(using a model of the environment) and importance sampling, combining the strengths of both approaches while 
mitigating their individual weaknesses.

### Integrating Doubly Robust Estimators with MCTS
We hypothesize that the **nice** properties of DR estimators can be preserved when integrated with MCTS. 
In particular, we propose a hybrid estimator that combines the rollout policy in MCTS with a DR estimator to
estimate the value function. This hybrid estimator leverages the strengths of both approaches:
1. The rollout policy provides a quick estimate of the value function, guiding the search towards promising states.
2. The DR estimator refines the value function estimate, reducing bias and variance in the estimation process.

Below is my attempt to rigorously justify the integration of DR estimators with MCTS.

#### Preliminaries
Let $$ \mathcal{S} $$ be the state space, $$ \mathcal{A} $$ the action space, and $$ \mathcal{H} $$ the space of 
action-observation histories. We define the following:


- $$ \pi_e: \mathcal{H} \times \mathcal{A} \rightarrow [0,1] $$: the target policy (MCTS policy)
- $$ \pi_b: \mathcal{H} \times \mathcal{A} \rightarrow [0,1] $$: the behavior policy (LLM-based heuristic policy)
- $$ Q: \mathcal{H} \times \mathcal{A} \rightarrow \mathbb{R} $$: the true state-action value function
- $$ \hat{Q}: \mathcal{H} \times \mathcal{A} \rightarrow \mathbb{R} $$: the estimated state-action value function
- $$ \hat{V}: \mathcal{H} \rightarrow \mathbb{R} $$: the estimated state value function
- $$ \gamma \in [0,1] $$: the discount factor
