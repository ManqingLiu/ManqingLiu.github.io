---
layout: post
title: On integrating doubly robust estimators with MCTS and it's connection with PPO
date: 2025-01-08
description: A blog post on integrating doubly robust estimators with MCTS and it's connection with PPO
tags: RL causality comments
categories: research work-in-progress
giscus_comments: true
related_posts: false
toc:
  beginning: true
wip: true
---

In this post, I will discuss my attempt to **theoretically** justify integrating doubly robust estimators in MCTS and
how it relates to [Proximal Policy Optimization (PPO)](https://arxiv.org/abs/1707.06347). Any feedback or comments are welcome!

### Problem Statement

Monte Carlo Tree Search (MCTS) is a popular algorithm for decision-making in large state spaces. It has been widely
used in algorithms like [AlphaGo](https://deepmind.google/research/breakthroughs/alphago/), [AlphaZero](https://deepmind.google/discover/blog/alphazero-shedding-new-light-on-chess-shogi-and-go/),
and [AlphaFold](https://deepmind.google/technologies/alphafold/). With the popularity of large language models (LLMs)
like [GPTs](https://openai.com/index/introducing-chatgpt-pro/), [Claude](https://claude.ai/new), and [LLama](https://www.llama.com/) in the NLP community,
there has been a growing interest in using LLMs as a world model in MCTS ([Zhao et al. 2023](https://arxiv.org/abs/2305.14078), [Zhou et al. 2024](https://arxiv.org/abs/2310.04406)
).
The idea is to use the LLM's world model to provide a prior belief of states for reasoned decision-making.
The value function of the MCTS is then used to guide the search to relevant parts of the tree. Traditionally, the value
function is estimated using a rollout policy. However, the rollout policy may not be accurate, especially in high-dimensional
state spaces. One way to improve the value function estimation is to use doubly robust estimators.

