---
layout: post
title: On integrating doubly robust estimators with MCTS 
date: 2025-01-08
description: A blog post on integrating doubly robust estimators with MCTS 
tags: RL causality comments
categories: research work-in-progress
giscus_comments: true
related_posts: false
toc:
  beginning: true
wip: true
---

In this post, I will explore the **theoretical** justification for integrating doubly robust estimators into Monte Carlo
Tree Search (MCTS). 
I welcome any feedback or comments on this post.

### Problem Statement

Monte Carlo Tree Search (MCTS) has emerged as a powerful algorithm for decision-making in complex, large-scale state spaces. 
Its success is evident in groundbreaking AI systems such as
[AlphaGo](https://deepmind.google/research/breakthroughs/alphago/), [AlphaZero](https://deepmind.google/discover/blog/alphazero-shedding-new-light-on-chess-shogi-and-go/),
and [AlphaFold](https://deepmind.google/technologies/alphafold/). With the recent advancements of large language models (LLMs)
like [GPTs](https://openai.com/index/introducing-chatgpt-pro/), [Claude](https://claude.ai/new), and [LLama](https://www.llama.com/) in the NLP community,
there has been a growing interest in leveraging these models with MCTS frameworks. 

Recent research ([Zhao et al. 2023](https://arxiv.org/abs/2305.14078), [Zhou et al. 2024](https://arxiv.org/abs/2310.04406)
) has explored the integration of LLMs into MCTS. It has been speculated that OpenAI's [o1 model](https://openai.com/o1/) employs MCTS, using the model's confidence to guide the search
and expand the solution space ([Zhao et al. 2024](https://arxiv.org/pdf/2411.14405v1)). The core idea is to use the LLM's world model to provide a prior belief of states, enabling more informed decision-making.

Traditionally, MCTS relies on a rollout policy to estimate the value function, which guides the search towards relevant 
parts of the tree. However, this approach may be less effective in high-dimensional state spaces where accurate rollouts 
are challenging to obtain. While MCTS is primarily an on-policy algorithm, techniques such as importance sampling (IS) 
and doubly robust (DR) estimators—typically associated with off-policy evaluation—have not been widely applied to MCTS.

Our research interest lies in integrating DR estimators into MCTS to enhance value function estimation and, 
consequently, improve the search's success rate. This integration presents an intriguing opportunity to bridge 
the gap between on-policy learning in MCTS and off-policy evaluation techniques.

### Why Doubly Robust Estimators?

[Doubly robust (DR) estimators](https://academic.oup.com/biomet/article-abstract/82/4/805/252258) have 
gained significant popularity in causal inference due to their unique properties. 
Originally developed for missing data problems, DR estimators have found widespread use in observational studies and 
clinical trials. The key advantage of DR estimators is their ability to provide unbiased estimates if either the 
outcome model or the propensity score model is correctly specified, offering a safeguard against model misspecification.

In causal inference, DR estimators combine two approaches:
1. An outcome regression model that predicts the outcome based on covariates and treatment.
2. A propensity score model that estimates the probability of receiving treatment given covariates.

This dual modeling approach provides a "double" chance of getting the correct estimate, hence the term "doubly robust."

The success of DR estimators in causal inference has inspired researchers to adapt these methods to the 
reinforcement learning (RL) domain. Two seminal papers have been instrumental in this transition:

1. [Jiang and Li (2016)](https://arxiv.org/abs/1511.03722) introduced the idea of using DR estimators for off-policy evaluation in RL. 
Their work demonstrated how DR estimators could be applied to estimate the value of a target policy using 
data collected from a different behavior policy. This approach showed promising results in reducing variance and 
bias compared to traditional importance sampling methods.

2. [Thomas and Brunskill (2016)](https://arxiv.org/abs/1604.00923) further developed this concept by proposing a more data-efficient DR estimator for 
off-policy evaluation. Their method, known as the weighted doubly robust estimator, incorporated adaptive importance 
sampling techniques to improve estimation accuracy, especially in scenarios with limited data.

These pioneering works established DR estimators as a powerful tool in RL, particularly for off-policy evaluation. 
The success of DR estimators in this context stems from their ability to leverage both the direct method 
(using a model of the environment) and importance sampling, combining the strengths of both approaches while 
mitigating their individual weaknesses.

### Integrating Doubly Robust Estimators with MCTS
We hypothesize that the nice properties of DR estimators can be preserved when integrated with MCTS. 
