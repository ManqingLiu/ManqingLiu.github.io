---
layout: post
title: On integrating doubly robust estimators with MCTS and it's connection with PPO
date: 2025-01-08
description: A blog post on integrating doubly robust estimators with MCTS and it's connection with PPO
tags: RL causality comments
categories: research work-in-progress
giscus_comments: true
related_posts: false
toc:
  beginning: true
wip: true
---

In this post, I will discuss my attempt to **theoretically** justify integrating doubly robust estimators in MCTS and
how it relates to [Proximal Policy Optimization (PPO)](https://arxiv.org/abs/1707.06347). Any feedback or comments are welcome!

### Problem Statement

Monte Carlo Tree Search (MCTS) is a popular algorithm for decision-making in large state spaces. It has been widely
used in algorithms like [AlphaGo](https://deepmind.google/research/breakthroughs/alphago/), [AlphaZero](https://deepmind.google/discover/blog/alphazero-shedding-new-light-on-chess-shogi-and-go/),
and [AlphaFold](https://deepmind.google/technologies/alphafold/). With the popularity of large language models (LLMs)
like [GPTs](https://openai.com/index/introducing-chatgpt-pro/), [Claude](https://claude.ai/new), and [LLama](https://www.llama.com/) in the NLP community,
there has been a growing interest in using LLMs as a world model in MCTS {% cite pmlr-v235-zhou24r, NEURIPS2023_65a39213 %}.
The idea is to use the LLM's world model to provide a prior belief of states for reasoned decision-making.
However, MCTS can be sample inefficient, especially in high-dimensional state spaces. One way to improve sample efficiency
is to use off-policy estimators like doubly robust estimators.

{% bibliography --cited %}
